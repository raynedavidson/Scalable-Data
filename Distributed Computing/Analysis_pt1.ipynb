{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function start_spark in module __main__:\n",
      "\n",
      "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)\n",
      "    Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
      "    \n",
      "    Args:\n",
      "        executor_instances (int): number of executors (default: 2)\n",
      "        executor_cores (int): number of cores per executor (default: 1)\n",
      "        worker_memory (float): worker memory (default: 1)\n",
      "        master_memory (float): master memory (default: 1)\n",
      "\n",
      "Help on function stop_spark in module __main__:\n",
      "\n",
      "stop_spark()\n",
      "    Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
      "\n",
      "Help on function display_spark in module __main__:\n",
      "\n",
      "display_spark()\n",
      "    Display the status of the active Spark session if one is currently running.\n",
      "\n",
      "Help on function show_as_html in module __main__:\n",
      "\n",
      "show_as_html(df, n=20)\n",
      "    Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
      "    \n",
      "    Args:\n",
      "        n (int): number of rows to show (default: 20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Print function docstrings\n",
    "\n",
    "help(start_spark)\n",
    "help(stop_spark)\n",
    "help(display_spark)\n",
    "help(show_as_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>mda205 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4821\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4821</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>64</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1682571717023</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>16</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/mda205/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20230427170158-0055</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/mda205/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>mda205 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>45305</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>8g</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>mda205 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=4, worker_memory=8, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/home/mda205/.local/lib/python3.7/site-packages/geopandas/_compat.py:115: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "# Write your imports and code here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import textwrap\n",
    "import glob\n",
    "import math as m\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "import shapely\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefile(filename, tablename):\n",
    "\n",
    "    name = \"mda205\"\n",
    "\n",
    "    data_path = f\"hdfs:///user/{name}/outputs/{filename}/\"\n",
    "\n",
    "\n",
    "    tablename.write.mode(\"overwrite\").csv(data_path)\n",
    "    \n",
    "    (tablename.write\n",
    "        .option(\"compression\", \"gzip\")\n",
    "        .mode(\"overwrite\")\n",
    "        .csv(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_stations = StructType([\n",
    "    StructField(\"Station_ID\", StringType(), True),\n",
    "    StructField(\"State_Code\", StringType(), True),\n",
    "    StructField(\"Country_Code\", StringType(), True),\n",
    "    StructField(\"Lat\", DoubleType(), True),\n",
    "    StructField(\"Lon\", DoubleType(), True),\n",
    "    StructField(\"Elevation\", IntegerType(), True),\n",
    "    StructField(\"Station_Name\", StringType(), True),\n",
    "    StructField(\"GSN_flag\", StringType(), True),\n",
    "    StructField(\"HCN_flag\", StringType(), True),\n",
    "    StructField(\"WMO_ID\", IntegerType(), True),\n",
    "    StructField(\"Country_Name\", StringType(), True),\n",
    "    StructField(\"State_Name\", StringType(), True),\n",
    "    StructField(\"Station_Elements\", StringType(), True),\n",
    "    StructField(\"FirstYear\", IntegerType(), True),\n",
    "    StructField(\"LastYear\", IntegerType(), True),\n",
    "    StructField(\"Element_Count\", IntegerType(), True),\n",
    "    StructField(\"Core_Element_Count\", IntegerType(), True),\n",
    "    StructField(\"Not_Core_Elements\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "stations=(\n",
    "    spark.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\", \"False\")\n",
    "    .option(\"inferSchema\", \"False\")\n",
    "    .schema(schema_stations)\n",
    "    .load(\"hdfs:///user/mda205/outputs/no_array_stations\"))\n",
    "\n",
    "schema_daily = StructType([\n",
    "    StructField(\"Station_ID\", StringType(), True),\n",
    "    StructField(\"DATE\", DateType(), True),\n",
    "    StructField(\"Element\", StringType(), True),\n",
    "    StructField(\"Element_Value\", IntegerType(), True),\n",
    "    StructField(\"MEASUREMENT_FLAG\", StringType(), True),\n",
    "    StructField(\"QUALITY_FLAG\", StringType(), True),\n",
    "    StructField(\"SOURCE_FLAG\", StringType(), True),\n",
    "    StructField(\"OBSERVATION_TIME\", StringType(), True),\n",
    "])\n",
    "\n",
    "#Q2 c\n",
    "\n",
    "#Bringing in the small directories \n",
    "states=(\n",
    "    spark.read.text(\"hdfs:///data/ghcnd/ghcnd-states.txt\"))\n",
    "\n",
    "states=states.withColumn(\n",
    "    'State_Code', F.substring('value', 1,2)).withColumn(\n",
    "    'State_Name', F.substring('value', 4,47))\n",
    "\n",
    "states=states.drop(F.col(\"value\"))\n",
    "#--------\n",
    "countries=(\n",
    "    spark.read.text(\"hdfs:///data/ghcnd/ghcnd-countries.txt\"))\n",
    "\n",
    "countries=countries.withColumn(\n",
    "    'Country_Code', F.substring('value', 1,2)).withColumn(\n",
    "    'Country_Name', F.substring('value', 4,60))\n",
    "\n",
    "countries=countries.drop(F.col(\"value\"))\n",
    "#--------\n",
    "inventory=(\n",
    "    spark.read.text(\"hdfs:///data/ghcnd/ghcnd-inventory.txt\"))\n",
    "\n",
    "inventory=inventory.withColumn(\n",
    "    'Station_ID', F.substring('value', 1,11)).withColumn(\n",
    "    'Lat', F.substring('value', 13,8)).withColumn(\n",
    "    'Lon', F.substring('value', 22,9)).withColumn(\n",
    "    'Element', F.substring('value', 32,4)).withColumn(\n",
    "    'First_Year', F.substring('value', 37,4)).withColumn(\n",
    "    'Last_Year', F.substring('value', 42,4))\n",
    "\n",
    "inventory=inventory.drop(F.col(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_broadcasted = F.broadcast(stations).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Q1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q1 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------+---------+---------+---------------+---------------+\n",
      "|station count|active in 2022 count|HCN count|CRN count|GSN count|GSN & CRN count|GSN & HCN count|\n",
      "+-------------+--------------------+---------+---------+---------+---------------+---------------+\n",
      "|       124247|                8448|     1218|      234|      991|           null|             15|\n",
      "+-------------+--------------------+---------+---------+---------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=stations.agg(\n",
    "    F.countDistinct(\"Station_ID\").alias(\"station count\"),\n",
    "    F.sum(F.when(\n",
    "        F.col(\"LastYear\")==2022, 1)).alias(\"active in 2022 count\"),\n",
    "    F.sum(F.when(\n",
    "        F.col(\"HCN_flag\")==\"HCN\", 1)).alias(\"HCN count\"),\n",
    "    F.sum(F.when(\n",
    "        F.col(\"HCN_flag\")==\"CRN\", 1)).alias(\"CRN count\"),\n",
    "    F.count(\"GSN_flag\").alias(\"GSN count\"),\n",
    "    F.sum(F.when(\n",
    "        (F.col(\"GSN_flag\")==\"GSN\") & \n",
    "        (F.col(\"HCN_flag\")==\"CRN\"), 1)).alias(\"GSN & CRN count\"),\n",
    "    F.sum(F.when(\n",
    "        (F.col(\"GSN_flag\")==\"GSN\") & \n",
    "        (F.col(\"HCN_flag\")==\"HCN\"), 1)).alias(\"GSN & HCN count\"))\n",
    "\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCount=(stations.groupBy([\"Country_Code\"])\n",
    "              .agg(\n",
    "                  F.countDistinct(\"Station_ID\").alias(\"Station_Count\"))\n",
    "             .sort([F.col(\"Country_Code\").asc()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|Country_Code|Station_Count|\n",
      "+------------+-------------+\n",
      "|          AC|            2|\n",
      "|          AE|            4|\n",
      "|          AF|            4|\n",
      "|          AG|           87|\n",
      "|          AJ|           66|\n",
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryCount.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+\n",
      "|Country_Code|        Country_Name|Station_Count|\n",
      "+------------+--------------------+-------------+\n",
      "|          AC|Antigua and Barbuda |            2|\n",
      "|          AE|United Arab Emira...|            4|\n",
      "|          AF|         Afghanistan|            4|\n",
      "|          AG|            Algeria |           87|\n",
      "|          AJ|         Azerbaijan |           66|\n",
      "+------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_stationCount=(countries\n",
    "              .join(countryCount, on=\"Country_Code\", how='left'))\n",
    "\n",
    "countries_stationCount.show(5)\n",
    "savefile(\"countries\", countries_stationCount) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "statesCount=(stations.groupBy([\"State_Code\"])\n",
    "              .agg(\n",
    "                  F.countDistinct(\"Station_ID\").alias(\"Station_Count\"))\n",
    "             .sort([F.col(\"State_Code\").asc()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|State_Code|          State_Name|Station_Count|\n",
      "+----------+--------------------+-------------+\n",
      "|        AB|             ALBERTA|         1444|\n",
      "|        AK|              ALASKA|         1034|\n",
      "|        AL|ALABAMA          ...|         1089|\n",
      "|        AR|            ARKANSAS|          926|\n",
      "|        AS|      AMERICAN SAMOA|           21|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states_stationsCount=(states\n",
    "              .join(statesCount, on=\"State_Code\", how='left')\n",
    "              .withColumn(\"Station_Count\", F.col(\"Station_Count\")))\n",
    "\n",
    "states_stationsCount.show(5)\n",
    "savefile(\"states\", states_stationsCount) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q1 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|southern hemisphere|US Territories|\n",
      "+-------------------+--------------+\n",
      "|              25337|           371|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=stations.agg(\n",
    "    F.sum(F.when(F.col(\"Lat\")<=0, 1)).alias(\"southern hemisphere\"),\n",
    "    F.sum(F.when((stations.Country_Name.contains(\"[United States]\")), 1)).alias(\"US Territories\"))\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between_two_points(lat_A, lon_A, lat_B, lon_B):\n",
    "    \n",
    "    # i tried to use geopy but it didnt seem to work when wrapped in the UDF \n",
    "    # this was from user Michael0x2a on stackoverflow with minor changes\n",
    "    \n",
    "    R = 6373.0 #earth's radius in km\n",
    "\n",
    "    #math likes radians...kind of annoying\n",
    "    lat1=m.radians(lat_A)\n",
    "    lon1=m.radians(lon_A)\n",
    "    lat2=m.radians(lat_B)\n",
    "    lon2=m.radians(lon_B)\n",
    "\n",
    "    #some equation off stackoverflow \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    \n",
    "    a = m.sin(dlat / 2)**2 + m.cos(lat1) * m.cos(lat2) * m.sin(dlon / 2)**2\n",
    "    c = 2*m.atan2(m.sqrt(a), m.sqrt(1 - a))\n",
    "\n",
    "    distance = R*c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping function in pyspark UDF\n",
    "distance_between_two_points_UDF=(\n",
    "    F.udf(distance_between_two_points, FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q2 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NZStations=stations_broadcasted.alias(\"stationsCopy\")\n",
    "\n",
    "# filter to NZ and reduce table to only needed columns\n",
    "NZStationsA= (NZStations.filter((F.col(\"Country_Code\")==\"NZ\"))\n",
    "              .select(\n",
    "                  F.col(\"Station_ID\"),\n",
    "                  F.col(\"Lat\").cast(FloatType()),\n",
    "                  F.col(\"Lon\").cast(FloatType())))\n",
    "\n",
    "NZStationsB=(NZStationsA\n",
    "             .withColumnRenamed(\"Station_ID\", \"ID_Station_B\")\n",
    "             .withColumnRenamed(\"Lat\", \"Lat_Station_B\")\n",
    "             .withColumnRenamed(\"Lon\", \"Lon_Station_B\"))\n",
    "\n",
    "# this might make less shuffling but also might not matter because stations is small\n",
    "NZStationsB_broadcasted = F.broadcast(NZStationsB) \n",
    "\n",
    "# joining tables\n",
    "NZstations_joined=(NZStationsA.crossJoin(NZStationsB_broadcasted)\n",
    "                   .withColumnRenamed(\"Station_ID\", \"ID_Station_A\")\n",
    "                   .withColumnRenamed(\"Lat\", \"Lat_Station_A\")\n",
    "                   .withColumnRenamed(\"Lon\", \"Lon_Station_A\"))\n",
    "\n",
    "# adding column of calculation of distance using UDF\n",
    "NZstations_distance_calculated=(NZstations_joined.withColumn(\"Distance_Between_Stations_km\", \n",
    "                                 distance_between_two_points_UDF(\n",
    "                                     F.col(\"Lat_Station_A\"), \n",
    "                                     F.col(\"Lon_Station_A\"),\n",
    "                                     F.col(\"Lat_Station_B\"),\n",
    "                                     F.col(\"Lon_Station_B\"))))\n",
    "\n",
    "# dropping where stations compared were identical, making single row with both station IDs\n",
    "NZstations_distance_calculated=(NZstations_distance_calculated\n",
    "                     .where(NZstations_distance_calculated.Distance_Between_Stations_km!=0)\n",
    "                     .withColumn(\"Station_IDs_compared\", \n",
    "                                 F.concat(F.col(\"ID_Station_A\"), \n",
    "                                          F.lit(\"__\"), \n",
    "                                          F.col(\"ID_Station_B\"))))\n",
    "\n",
    "NZstations_pairwise_distances=(NZstations_distance_calculated\n",
    "            .select(\n",
    "                F.col(\"Station_IDs_compared\"), \n",
    "                F.col(\"Distance_Between_Stations_km\")\n",
    "            ))\n",
    "\n",
    "# look, its 2 weeks after i did this whole bit and i'm now aware at how....unnecesary \n",
    "# I made it, I fixed some but I'm not redoing the whole thing at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------------------+\n",
      "|    Station_IDs_compared|Distance_Between_Stations_km|\n",
      "+------------------------+----------------------------+\n",
      "|NZ000933090__NZ000939450|                   1553.7822|\n",
      "|NZ000933090__NZM00093929|                    1417.352|\n",
      "|NZ000933090__NZ000093844|                    951.2242|\n",
      "|NZ000933090__NZ000093417|                   220.26935|\n",
      "|NZ000933090__NZM00093781|                    516.1928|\n",
      "+------------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NZstations_pairwise_distances.show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savefile(\"NZ_stations_pairwise_distances\", NZstations_pairwise_distances) \n",
    "# dont need to keep saving when running book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+\n",
      "| Farthest_apart_stations|max_distance|\n",
      "+------------------------+------------+\n",
      "|NZ000939450__NZ000093994|   2800.0547|\n",
      "+------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=(NZstations_pairwise_distances\n",
    "   .orderBy(\n",
    "       F.col(\"Distance_Between_Stations_km\").desc()).limit(1)\n",
    "   .withColumnRenamed(\"Distance_Between_Stations_km\", \"max_distance\")\n",
    "   .withColumnRenamed(\"Station_IDs_compared\", \"Farthest_apart_stations\"))\n",
    "   \n",
    "y.show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q3 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey \"dfs.blocksize\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=mda205&files=1&blocks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2023.csv.gz\n",
      "FSCK started by mda205 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2023.csv.gz at Thu Apr 27 17:02:43 NZST 2023\n",
      "\n",
      "/data/ghcnd/daily/2023.csv.gz 27521531 bytes, replicated: replication=8, 1 block(s):  OK\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1073824428_83608 len=27521531 Live_repl=8\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t32\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t27521531 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 27521531 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t4\n",
      " Average block replication:\t8.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Thu Apr 27 17:02:43 NZST 2023 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/ghcnd/daily/2023.csv.gz' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck hdfs:///data/ghcnd/daily/2023.csv.gz -files -blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=mda205&files=1&blocks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2022.csv.gz\r\n",
      "FSCK started by mda205 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2022.csv.gz at Thu Apr 27 17:02:46 NZST 2023\r\n",
      "\r\n",
      "/data/ghcnd/daily/2022.csv.gz 166075423 bytes, replicated: replication=8, 2 block(s):  OK\r\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1073824426_83606 len=134217728 Live_repl=8\r\n",
      "1. BP-700027894-132.181.129.68-1626517177804:blk_1073824427_83607 len=31857695 Live_repl=8\r\n",
      "\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t32\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t0\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t166075423 B\r\n",
      " Total files:\t1\r\n",
      " Total blocks (validated):\t2 (avg. block size 83037711 B)\r\n",
      " Minimally replicated blocks:\t2 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t0 (0.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t4\r\n",
      " Average block replication:\t8.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t0 (0.0 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Thu Apr 27 17:02:46 NZST 2023 in 1 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/data/ghcnd/daily/2022.csv.gz' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck hdfs:///data/ghcnd/daily/2022.csv.gz -files -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q3 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_data_for_select_year(startyear, endyear=False):\n",
    "    \n",
    "    \"\"\"retreive data for specified year or years. if only looking at one year,\n",
    "    startyear is treated as target year. enter years as integers and in chronological order\n",
    "    endyear defaults to false if no second argument is given\"\"\"\n",
    "    \n",
    "    if endyear != False:\n",
    "        \n",
    "        years=range(startyear, endyear + 1)\n",
    "        \n",
    "        # ChatGPT helped with the join part\n",
    "        yearsconcat=\"{\"+\",\".join([f\"{year}\" for year in years])+\"}*\"\n",
    "        \n",
    "        filepath=(\"hdfs:///data/ghcnd/daily/\"+yearsconcat+\"*\")\n",
    "       \n",
    "    elif endyear==False:\n",
    "        targetYear=f'{startyear}'\n",
    "        \n",
    "        filepath=(\"hdfs:///data/ghcnd/daily/\"+targetYear+\"*\")\n",
    "\n",
    "    data=(\n",
    "        spark.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .option(\"dateFormat\", \"yyyymmdd\")\n",
    "        .schema(schema_daily).load(filepath))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|# of observations in 2022|\n",
      "+-------------------------+\n",
      "|                 37375779|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2022=get_daily_data_for_select_year(2022)\n",
    "\n",
    "k=data2022.select(F.count(data2022.Element).alias(\"# of observations in 2022\"))\n",
    "\n",
    "k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|# of observations in 2023|\n",
      "+-------------------------+\n",
      "|                  6031842|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2023=get_daily_data_for_select_year(2023)\n",
    "\n",
    "r=data2023.select(F.count(data2023.Element).alias(\"# of observations in 2023\"))\n",
    "\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Q3 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|# of observations since 2014|\n",
      "+----------------------------+\n",
      "|                   337279894|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2014_23=get_daily_data_for_select_year(2014, 2023)\n",
    "\n",
    "w=data2014_23.select(F.count(data2014_23.Element).alias(\"# of observations since 2014\"))\n",
    "\n",
    "w.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Q4 has it's own notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
