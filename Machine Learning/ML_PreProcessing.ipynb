{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function start_spark in module __main__:\n",
      "\n",
      "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)\n",
      "    Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
      "    \n",
      "    Args:\n",
      "        executor_instances (int): number of executors (default: 2)\n",
      "        executor_cores (int): number of cores per executor (default: 1)\n",
      "        worker_memory (float): worker memory (default: 1)\n",
      "        master_memory (float): master memory (default: 1)\n",
      "\n",
      "Help on function stop_spark in module __main__:\n",
      "\n",
      "stop_spark()\n",
      "    Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
      "\n",
      "Help on function display_spark in module __main__:\n",
      "\n",
      "display_spark()\n",
      "    Display the status of the active Spark session if one is currently running.\n",
      "\n",
      "Help on function show_as_html in module __main__:\n",
      "\n",
      "show_as_html(df, n=20)\n",
      "    Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
      "    \n",
      "    Args:\n",
      "        n (int): number of rows to show (default: 20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Print function docstrings\n",
    "\n",
    "help(start_spark)\n",
    "help(stop_spark)\n",
    "help(display_spark)\n",
    "help(show_as_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>mda205 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4281\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>64</td></tr><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/mda205/DATA420%20Assignment%202/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20230608192650-0487</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>16</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>46185</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4281</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/mda205/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1686209208791</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>mda205 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>mda205 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=4, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports and code here or insert cells below\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from pretty import SparkPretty  # download pretty.py from LEARN and put it in your M:\\ or home directory\n",
    "pretty = SparkPretty(limit=5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this isnt an answer for Q1 but just a note to you -- i know you commented on my style/readability on the last assignment and because of that i am taking COSC480 next semester (all my knowledge on actually writing code is just duct taped together) but the good news is the majority of this assignment is copy and pastes of your code so much less of this is my bad style :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ideal # partitions = 64\n"
     ]
    }
   ],
   "source": [
    "# Determine ideal number of partitions\n",
    "\n",
    "conf = sc.getConf()\n",
    "\n",
    "N = int(conf.get(\"spark.executor.instances\"))\n",
    "M = int(conf.get(\"spark.executor.cores\"))\n",
    "partitions = 4 * N * M\n",
    "\n",
    "print(f'ideal # partitions = {partitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_Datasets_counts():\n",
    "    \n",
    "    for name in datasets:\n",
    "\n",
    "        metadata_schema = StructType([\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"type\", StringType()),\n",
    "        ])\n",
    "        metadata = spark.read.csv(\n",
    "            f'/data/msd/audio/attributes/{name}.attributes.csv', schema=metadata_schema)\n",
    "\n",
    "        schema_actual = StructType([\n",
    "            StructField(name, lookup[typename], True) for name, typename in metadata.collect()])\n",
    "\n",
    "        features = spark.read.csv(f'/data/msd/audio/features/{name}.csv', \n",
    "                              schema=schema_actual, quote=\"'\").withColumnRenamed(schema_actual[-1].name, \"track_id\")\n",
    "\n",
    "        print(name+\" has:\")\n",
    "        print(f\"{len(features.columns)} columns\")\n",
    "        print(f\"{features.count()} rows\")\n",
    "        df=features.select((F.countDistinct(\"track_id\")).alias(\"total unique songs\"))\n",
    "        df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_Datasets_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 (All you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   8 jsw93 supergroup     11.1 M 2021-09-29 10:35 hdfs:///data/msd/genre/msd-MAGD-genreAssignment.tsv\r\n",
      "-rw-r--r--   8 jsw93 supergroup      8.4 M 2021-09-29 10:35 hdfs:///data/msd/genre/msd-MASD-styleAssignment.tsv\r\n",
      "-rw-r--r--   8 jsw93 supergroup     10.6 M 2021-09-29 10:35 hdfs:///data/msd/genre/msd-topMAGD-genreAssignment.tsv\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -ls -h hdfs:///data/msd/genre/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9d8\r\n",
      "< ERROR: <SOFQHZM12A8C142342 TRMWMFG128F92FFEF2> Josipa Lisac  -  razloga  !=  Lisac Josipa  -  1000 razloga\r\n",
      "19d17\r\n",
      "< ERROR: <SODXUTF12AB018A3DA TRMWPCD12903CCE5ED> Lutan Fyah  -  Nuh Matter the Crisis Feat. Midnite  !=  Midnite  -  Nah Matter the Crisis\r\n",
      "29d26\r\n",
      "< ERROR: <SOASCRF12A8C1372E6 TRMHIPJ128F426A2E2> Gaetano Donizetti  -  L'Elisir d'Amore: Act Two: Come sen va contento!  !=  Gianandrea Gavazzeni_ Orchestra E Coro Del Maggio Musicale Fiorentino_ Carlo Bergonzi_ Renata Scotto  -  L'Elisir D'Amore_ Act 2: Come Sen Va Contento (Adina) (Donizetti)\r\n",
      "33d29\r\n",
      "< ERROR: <SOITDUN12A58A7AACA TRMHXGK128F42446AB> C.J. Chenier  -  Ay, Ai Ai  !=  Clifton Chenier  -  Ay_ Ai Ai\r\n",
      "52d47\r\n",
      "< ERROR: <SOLZXUM12AB018BE39 TRMRSOF12903CCF516> 許志安  -  男人最痛  !=  Andy Hui  -  Nan Ren Zui Tong\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -cat /data/msd/tasteprofile/mismatches/sid_matches_manually_accepted.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: <SOUMNSI12AB0182807 TRMMGKQ128F9325E10> Digital Underground  -  The Way We Swing  !=  Linkwood  -  Whats up with the Underground\r\n",
      "ERROR: <SOCMRBE12AB018C546 TRMMREB12903CEB1B1> Jimmy Reed  -  The Sun Is Shining (Digitally Remastered)  !=  Slim Harpo  -  I Got Love If You Want It\r\n",
      "ERROR: <SOLPHZY12AC468ABA8 TRMMBOC12903CEB46E> Africa HiTech  -  Footstep  !=  Marcus Worgull  -  Drumstern (BONUS TRACK)\r\n",
      "ERROR: <SONGHTM12A8C1374EF TRMMITP128F425D8D0> Death in Vegas  -  Anita Berber  !=  Valen Hsu  -  Shi Yi\r\n",
      "ERROR: <SONGXCA12A8C13E82E TRMMAYZ128F429ECE6> Grupo Exterminador  -  El Triunfador  !=  I Ribelli  -  Lei M'Ama\r\n",
      "ERROR: <SOMBCRC12A67ADA435 TRMMNVU128EF343EED> Fading Friend  -  Get us out!  !=  Masterboy  -  Feel The Heat 2000\r\n",
      "ERROR: <SOTDWDK12A8C13617B TRMMNCZ128F426FF0E> Daevid Allen  -  Past Lives  !=  Bhimsen Joshi  -  Raga - Shuddha Sarang_ Aalap\r\n",
      "ERROR: <SOEBURP12AB018C2FB TRMMPBS12903CE90E1> Cristian Paduraru  -  Born Again  !=  Yespiring  -  Journey Stages\r\n",
      "ERROR: <SOSRJHS12A6D4FDAA3 TRMWMEL128F421DA68> Jeff Mills  -  Basic Human Design  !=  M&T  -  Drumsettester\r\n",
      "ERROR: <SOIYAAQ12A6D4F954A TRMWHRI128F147EA8E> Excepter  -  OG  !=  The Fevers  -  Não Tenho Nada (Natchs Scheint Die Sonne)\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -cat /data/msd/tasteprofile/mismatches/sid_mismatches.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches_schema = StructType([\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"song_artist\", StringType(), True),\n",
    "    StructField(\"song_title\", StringType(), True),\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "    StructField(\"track_artist\", StringType(), True),\n",
    "    StructField(\"track_title\", StringType(), True)\n",
    "])\n",
    "\n",
    "path = \"/scratch-network/courses/2023/DATA420-23S1/data/msd/tasteprofile/mismatches/sid_matches_manually_accepted.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    sid_matches_manually_accepted = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"< ERROR: \"):\n",
    "            a = line[10:28]\n",
    "            b = line[29:47]\n",
    "            c, d = line[49:-1].split(\"  !=  \")\n",
    "            e, f = c.split(\"  -  \")\n",
    "            g, h = d.split(\"  -  \")\n",
    "            sid_matches_manually_accepted.append((a, e, f, b, g, h))\n",
    "\n",
    "matches_manually_accepted = spark.createDataFrame(sc.parallelize(sid_matches_manually_accepted, 8), schema=mismatches_schema)\n",
    "# show_as_html(matches_manually_accepted)\n",
    "\n",
    "path = \"/scratch-network/courses/2023/DATA420-23S1/data/msd/tasteprofile/mismatches/sid_mismatches.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    sid_mismatches = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"ERROR: \"):\n",
    "            a = line[8:26]\n",
    "            b = line[27:45]\n",
    "            c, d = line[47:-1].split(\"  !=  \")\n",
    "            e, f = c.split(\"  -  \")\n",
    "            g, h = d.split(\"  -  \")\n",
    "            sid_mismatches.append((a, e, f, b, g, h))\n",
    "\n",
    "mismatches = spark.createDataFrame(sc.parallelize(sid_mismatches, 64), schema=mismatches_schema)\n",
    "# show_as_html(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse triplets in spark\n",
    "\n",
    "triplets_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"plays\", IntegerType(), True)\n",
    "])\n",
    "triplets = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .option(\"codec\", \"gzip\")\n",
    "    .schema(triplets_schema)\n",
    "    .load(\"hdfs:///data/msd/tasteprofile/triplets.tsv/\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches_manually_accepted = 488\n",
      "mismatches                = 19094\n",
      "triplets                  = 48373586\n",
      "triplets_not_mismatched   = 45795111\n"
     ]
    }
   ],
   "source": [
    "# Anti join mismatches to manually accepted, and anti join the remaining mismatches to triplets\n",
    "\n",
    "mismatches_not_accepted = mismatches.join(matches_manually_accepted, on=\"song_id\", how=\"left_anti\")\n",
    "triplets_not_mismatched = triplets.join(mismatches_not_accepted, on=\"song_id\", how=\"left_anti\")\n",
    "\n",
    "print(f\"matches_manually_accepted = {matches_manually_accepted.count()}\")\n",
    "print(f\"mismatches                = {mismatches.count()}\")\n",
    "print(f\"triplets                  = {triplets.count()}\")\n",
    "print(f\"triplets_not_mismatched   = {triplets_not_mismatched.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset names and attribute type mapping\n",
    "\n",
    "attributes_path = 'hdfs:///data/msd/audio/attributes/'\n",
    "\n",
    "hadoop = sc._jvm.org.apache.hadoop\n",
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration()\n",
    "path = hadoop.fs.Path(attributes_path)\n",
    "getfile=fs.get(conf).listStatus(path)\n",
    "\n",
    "datasets=[]\n",
    "modified_filenames=[]\n",
    "\n",
    "for f in getfile:\n",
    "    \n",
    "    filename = f.getPath().getName()\n",
    "    datasets.append(filename[:-15])\n",
    "    \n",
    "    mod_filename = filename.replace(\"-\",\"_\")\n",
    "    modified_filenames.append(mod_filename[:-17])\n",
    "    \n",
    "\n",
    "lookup = {\n",
    "    'real': DoubleType(),\n",
    "    'NUMERIC': DoubleType(),\n",
    "    'float': DoubleType(),\n",
    "    'string': StringType(),\n",
    "    'STRING': StringType(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose a dataset name, load attribute names, and define schemas based on attribute names\n",
    "\n",
    "def SingleDataset(dataname):\n",
    "\n",
    "    name = f'{dataname}'\n",
    "\n",
    "    metadata_schema = StructType([\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"type\", StringType())])\n",
    "\n",
    "    metadata = spark.read.csv(f'/data/msd/audio/attributes/{name}.attributes.csv', schema=metadata_schema)\n",
    "\n",
    "    schema_actual = StructType([\n",
    "        StructField(name, lookup[typename], True) for name, typename in metadata.collect()])\n",
    "\n",
    "    SingleData = spark.read.csv(f'/data/msd/audio/features/{name}.csv', \n",
    "                                schema=schema_actual, quote=\"'\").withColumnRenamed(schema_actual[-1].name, \"track_id\")\n",
    "    \n",
    "    \n",
    "    def DFCounts(name, df):\n",
    "        print(name+\" has:\")\n",
    "        print(f\"{len(df.columns)} columns\")\n",
    "        print(f\"{df.count()} rows\")\n",
    "        df=df.select((F.countDistinct(\"track_id\")).alias(\"total unique songs\"))\n",
    "        df.show()\n",
    "    \n",
    "    counts = lambda : DFCounts(name, SingleData) # printing this information is optional by calling count.()\n",
    "    \n",
    "    return SingleData, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole notebook pretty much just makes functions to use later (maybe) i guess. or at least my contribution to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>mda205 (jupyter)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
